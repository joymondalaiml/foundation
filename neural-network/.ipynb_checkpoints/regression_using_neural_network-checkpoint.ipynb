{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f1677d",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Machine Learning with Python Cookbook, 2nd Edition, Kyle Gallatin, Chris Albon, O'Reilly Media, Inc.\n",
    "- http://alexlenail.me/NN-SVG/index.html\n",
    "- https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\n",
    "- https://learn.udacity.com/courses/ud187\n",
    "- https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "- https://aws.amazon.com/what-is/neural-network/\n",
    "- https://thedatascientist.com/what-deep-learning-is-and-isnt/\n",
    "- https://www.ibm.com/topics/gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1129867",
   "metadata": {},
   "source": [
    "### Traditional programming vs machine learning:\n",
    "\n",
    "<img src=\"./traditional_vs_machine_learning1.png\" align=\"center\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0d496",
   "metadata": {},
   "source": [
    "### What is an artificial neural network?\n",
    "\n",
    "A artificial neural network is a method in artificial intelligence that teaches computers to process data in a way that is **inspired by the human brain**. It is a type of machine learning process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain.\n",
    "\n",
    "Neural networks can help computers make intelligent decisions with limited human assistance. This is because they can **learn and model the relationships between input and output data that are nonlinear and complex**.\n",
    "\n",
    "Reference : https://aws.amazon.com/what-is/neural-network/ <br>\n",
    "\n",
    "<img src=\"./Neuron3.png\" align=\"center\" width=\"500\" />\n",
    "\n",
    "Image credit : https://en.wikipedia.org/wiki/Artificial_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db18e2c",
   "metadata": {},
   "source": [
    "### From artificial neural network to 'deep' artificial neural network\n",
    "\n",
    "<img src=\"./simple_vs_deep_neural_network.png\" align=\"center\" width=\"900\" />\n",
    "\n",
    "- **Input layer** : Information from outside the world enters into the artifical neural network from the input layer.\n",
    "- **Hidden layer** : Hidden layers take their input from the input layer or other hidden layers. Artificial neural networks can have a large number of hidden layers. Each hidden layer analyzes the output from the previous layer, processes it further, and passes it on to the next layer.\n",
    "- **Output layer** : The output layer gives the final result of all the data processing by the artificial neural network. It can have single or multiple nodes. For instance, if we have a binary (yes/no) classification problem, the output layer will have one output node, which will give the result as 1 or 0. However, if we have a multi-class classification problem, the output layer might consist of more than one output node.\n",
    "\n",
    "Image credit : https://thedatascientist.com/what-deep-learning-is-and-isnt/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6adf3",
   "metadata": {},
   "source": [
    "### Traditional machine learning vs deep learning\n",
    "\n",
    "<img src=\"./machine_learning_vs_deep_learning.png\" align=\"center\" width=\"800\" />\n",
    "\n",
    "- In traditional machine learning, a data scientist needs to determine the set of features which the model would learn from.\n",
    "- In deep learning the model derives the features itself and learns independently. Deep learning shines in case of unstructured data e.g., image, text, audio etc.\n",
    "\n",
    "Image credit : https://thedatascientist.com/what-deep-learning-is-and-isnt/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af4887d",
   "metadata": {},
   "source": [
    "### Let's look inside an individual artificial neural network node:\n",
    "\n",
    "We are targeting to find the rule to convert temperature in Celsius scale to Fahrenheit which is mentioned above using a one node neural network.\n",
    "\n",
    "<img src=\"./neural_network_one_node1.png\" align=\"center\" width=\"1000\" />\n",
    "\n",
    "Image credit : https://www.ibm.com/topics/gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a0283",
   "metadata": {},
   "source": [
    "### Building a full blown artificial neural network from the concept mentioned above:\n",
    "\n",
    "<img src=\"./neural_network_nodes1.png\" align=\"center\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c4fb7",
   "metadata": {},
   "source": [
    "### Training an artifical neural network:\n",
    "\n",
    "<img src=\"./neural_network_training.png\" align=\"center\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327f32a",
   "metadata": {},
   "source": [
    "### It's time for coding :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad887868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all the necessary libraries at the beginning\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop \n",
    "from sklearn.datasets import make_regression \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cd0575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "Using device: cuda\n",
      "\n",
      "True\n",
      "1\n",
      "<torch.cuda.device object at 0x0000020B0A98D4F0>\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using CUDA\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c55d5d",
   "metadata": {},
   "source": [
    "### What is CUDA?\n",
    "CUDA is a programming model and computing toolkit developed by NVIDIA. It enables you to perform compute-intensive operations faster by parallelizing tasks across GPUs. CUDA is the dominant API used for deep learning although other options are available, such as OpenCL. PyTorch provides support for CUDA in the torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3292d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate a sample regression dataset\n",
    "features, target = make_regression(n_features = 5, n_samples = 1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7867c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.433935</td>\n",
       "      <td>1.031306</td>\n",
       "      <td>-2.965524</td>\n",
       "      <td>0.134074</td>\n",
       "      <td>0.697376</td>\n",
       "      <td>-68.741145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.109711</td>\n",
       "      <td>0.867638</td>\n",
       "      <td>0.069104</td>\n",
       "      <td>-1.912093</td>\n",
       "      <td>-0.061461</td>\n",
       "      <td>48.917565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046456</td>\n",
       "      <td>0.541363</td>\n",
       "      <td>-1.058704</td>\n",
       "      <td>-0.907559</td>\n",
       "      <td>-0.526042</td>\n",
       "      <td>-76.572045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.191237</td>\n",
       "      <td>0.705107</td>\n",
       "      <td>-1.787081</td>\n",
       "      <td>0.806705</td>\n",
       "      <td>-0.404556</td>\n",
       "      <td>-79.933531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.435432</td>\n",
       "      <td>-1.125913</td>\n",
       "      <td>0.215511</td>\n",
       "      <td>-1.070098</td>\n",
       "      <td>-0.740660</td>\n",
       "      <td>-122.376190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4           5\n",
       "0 -0.433935  1.031306 -2.965524  0.134074  0.697376  -68.741145\n",
       "1  0.109711  0.867638  0.069104 -1.912093 -0.061461   48.917565\n",
       "2 -0.046456  0.541363 -1.058704 -0.907559 -0.526042  -76.572045\n",
       "3 -0.191237  0.705107 -1.787081  0.806705 -0.404556  -79.933531\n",
       "4  0.435432 -1.125913  0.215511 -1.070098 -0.740660 -122.376190"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look into top few records from the training dataset\n",
    "pd.DataFrame(np.hstack((features_train, target_train.reshape(-1,1)))).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b172a031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.732280</td>\n",
       "      <td>-1.648323</td>\n",
       "      <td>2.318695</td>\n",
       "      <td>0.917547</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>79.127472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009549</td>\n",
       "      <td>-0.154502</td>\n",
       "      <td>-1.509998</td>\n",
       "      <td>-0.327571</td>\n",
       "      <td>-0.179232</td>\n",
       "      <td>-121.631682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.178757</td>\n",
       "      <td>-0.719495</td>\n",
       "      <td>-0.209334</td>\n",
       "      <td>-1.550079</td>\n",
       "      <td>-1.714379</td>\n",
       "      <td>-242.437942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.967328</td>\n",
       "      <td>-0.557492</td>\n",
       "      <td>-0.278034</td>\n",
       "      <td>-0.447881</td>\n",
       "      <td>-1.378088</td>\n",
       "      <td>-324.937476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.151556</td>\n",
       "      <td>0.859110</td>\n",
       "      <td>-0.590210</td>\n",
       "      <td>-0.222043</td>\n",
       "      <td>-1.841062</td>\n",
       "      <td>-118.739455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4           5\n",
       "0  0.732280 -1.648323  2.318695  0.917547  0.100946   79.127472\n",
       "1  0.009549 -0.154502 -1.509998 -0.327571 -0.179232 -121.631682\n",
       "2 -0.178757 -0.719495 -0.209334 -1.550079 -1.714379 -242.437942\n",
       "3 -1.967328 -0.557492 -0.278034 -0.447881 -1.378088 -324.937476\n",
       "4 -0.151556  0.859110 -0.590210 -0.222043 -1.841062 -118.739455"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look into top few records from the test dataset\n",
    "pd.DataFrame(np.hstack((features_test, target_test.reshape(-1,1)))).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50b6832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature set -> type - <class 'numpy.ndarray'>, shape - (900, 5)\n",
      "[-0.43393538  1.03130648 -2.96552365  0.13407387  0.697376  ]\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the dataset\n",
    "print('Training feature set -> type - {}, shape - {}'.format(type(features_train), features_train.shape))\n",
    "print(features_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17661b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test feature set -> type - <class 'numpy.ndarray'>, shape - (100, 5)\n",
      "[ 0.73228021 -1.64832261  2.31869477  0.91754668  0.10094596]\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the dataset\n",
    "print('Test feature set -> type - {}, shape - {}'.format(type(features_test), features_test.shape))\n",
    "print(features_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9a9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target set -> type - <class 'numpy.ndarray'>, shape - (900,)\n",
      "-68.74114460661815\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the dataset\n",
    "print('Training target set -> type - {}, shape - {}'.format(type(target_train), target_train.shape))\n",
    "print(target_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70d391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test target set -> type - <class 'numpy.ndarray'>, shape - (100,)\n",
      "79.1274718490457\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the dataset\n",
    "print('Test target set -> type - {}, shape - {}'.format(type(target_test), target_test.shape))\n",
    "print(target_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c80a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with Pytorch, we need to convert the ndarrays to tensors\n",
    "x_train = torch.from_numpy(features_train).float() \n",
    "y_train = torch.from_numpy(target_train).float().view(-1,1) \n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3a732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature set -> type - <class 'torch.Tensor'>, shape - torch.Size([900, 5]), is CUDA - False\n",
      "tensor([-0.4339,  1.0313, -2.9655,  0.1341,  0.6974])\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the converted dataset\n",
    "print('Training feature set -> type - {}, shape - {}, is CUDA - {}'.format(type(x_train), x_train.shape, x_train.is_cuda))\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c24592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test feature set -> type - <class 'torch.Tensor'>, shape - torch.Size([100, 5]), is CUDA - False\n",
      "tensor([ 0.7323, -1.6483,  2.3187,  0.9175,  0.1009])\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the converted dataset\n",
    "print('Test feature set -> type - {}, shape - {}, is CUDA - {}'.format(type(x_test), x_test.shape, x_test.is_cuda))\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e58af3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train target set -> type - <class 'torch.Tensor'>, shape - torch.Size([900, 1]), is CUDA - False\n",
      "tensor([-68.7411])\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the converted dataset\n",
    "print('Train target set -> type - {}, shape - {}, is CUDA - {}'.format(type(y_train), y_train.shape, y_train.is_cuda))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801753a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test target set -> type - <class 'torch.Tensor'>, shape - torch.Size([100, 1]), is CUDA - False\n",
      "tensor([79.1275])\n"
     ]
    }
   ],
   "source": [
    "# Print the type and shape of the converted dataset\n",
    "print('Test target set -> type - {}, shape - {}, is CUDA - {}'.format(type(y_test), y_test.shape, y_test.is_cuda))\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d49cae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test target set -> type - <class 'torch.Tensor'>, shape - torch.Size([100, 1]), is CUDA - True\n",
      "tensor([79.1275], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Now, send all the datasets to GPU, if available\n",
    "x_train = x_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# Print the type and shape of the converted dataset\n",
    "print('Test target set -> type - {}, shape - {}, is CUDA - {}'.format(type(y_test), y_test.shape, y_test.is_cuda))\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c45e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Netowrk to solve this regression problem\n",
    "class RegressorNeuralNet(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(RegressorNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 10) # in_features=5, out_features=10\n",
    "        self.fc2 = nn.Linear(10, 10) # in_features=10, out_features=10\n",
    "        self.fc3 = nn.Linear(10, 1)  # in_features=10, out_features=1\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = nn.functional.relu(self.fc1(x))    # We used RELU as the activation function\n",
    "        x = nn.functional.relu(self.fc2(x))    # We used RELU as the activation function\n",
    "        x = self.fc3(x)                        # No activation function at the output layer as this is a regression problem\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0ad8e",
   "metadata": {},
   "source": [
    "### Activation function:\n",
    "- The purpose of an activation function is to introduce **non-linearity** into the output of a neuron.\n",
    "- Following are few popular activation functions for hidden layers:\n",
    "    - Rectified linear activation function or RELU\n",
    "    - Sigmoid\n",
    "    - TanH\n",
    "- Following are few popular activation functions for output layers:\n",
    "    - Linear\n",
    "    - Sigmoid\n",
    "    - Softmax\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "      <td>\n",
    "      <img src='./Linear.png' width=300>\n",
    "      </td>\n",
    "      <td>\n",
    "      <img src='./RELU.png' width=300>\n",
    "      </td>\n",
    "     </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "      Linear\n",
    "      </td>\n",
    "      <td>\n",
    "      RELU\n",
    "      </td>\n",
    "     </tr>  \n",
    "    <tr>\n",
    "      <td>\n",
    "      <img src='./Sigmoid.png' width=300>\n",
    "      </td>\n",
    "      <td>\n",
    "      <img src='./TanH.png' width=300>\n",
    "      </td>\n",
    "     </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "      Sigmoid\n",
    "      </td>\n",
    "      <td>\n",
    "      TanH\n",
    "      </td>\n",
    "     </tr>     \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d07b265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network now\n",
    "network = RegressorNeuralNet()\n",
    "\n",
    "# Define loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = RMSprop(network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99c1a9",
   "metadata": {},
   "source": [
    "## Calculating loss for regression problems:\n",
    "### We need to know how close are the predictions of the network to the actual targets\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error, this is in the square of the target value, so not very intuititive\n",
    "- Root Mean Squared Error, this is in the same unit as the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b061ae6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressorNeuralNet(\n",
       "  (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the network\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5919356",
   "metadata": {},
   "source": [
    "<img src=\"./regressor_neural_network.png\" align=\"left\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "044dad53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressorNeuralNet(\n",
       "  (fc1): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the network to GPU as well, if available\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba10a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the dataloader\n",
    "train_data = TensorDataset(x_train, y_train) \n",
    "train_loader = DataLoader(train_data, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "158cfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of EPOCHS we want to train\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67bd459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 13851.1435546875\n",
      "Epoch: 2 \tLoss: 6004.14990234375\n",
      "Epoch: 3 \tLoss: 2299.4560546875\n",
      "Epoch: 4 \tLoss: 662.6420288085938\n",
      "Epoch: 5 \tLoss: 363.2129211425781\n",
      "Epoch: 6 \tLoss: 257.5631408691406\n",
      "Epoch: 7 \tLoss: 333.099609375\n",
      "Epoch: 8 \tLoss: 232.1615447998047\n",
      "Epoch: 9 \tLoss: 196.04981994628906\n",
      "Epoch: 10 \tLoss: 201.78790283203125\n",
      "Epoch: 11 \tLoss: 179.89122009277344\n",
      "Epoch: 12 \tLoss: 185.03121948242188\n",
      "Epoch: 13 \tLoss: 170.4311065673828\n",
      "Epoch: 14 \tLoss: 153.2260284423828\n",
      "Epoch: 15 \tLoss: 143.64537048339844\n",
      "Epoch: 16 \tLoss: 117.62078857421875\n",
      "Epoch: 17 \tLoss: 97.60050201416016\n",
      "Epoch: 18 \tLoss: 115.50859832763672\n",
      "Epoch: 19 \tLoss: 113.16426849365234\n",
      "Epoch: 20 \tLoss: 82.55403137207031\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): \n",
    "        optimizer.zero_grad()  # This is very important to zero out the gradient before every iterations\n",
    "        output = network(data) # This is where we are performing the forward propagation\n",
    "        loss = criterion(output, target) # This is where we are calculating the loss\n",
    "        loss.backward() # This is where we are performing the backward propagation\n",
    "        optimizer.step() # This is where we update the network parameters\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddefae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 87.18633270263672\n"
     ]
    }
   ],
   "source": [
    "# Evaluate neural network \n",
    "with torch.no_grad(): # When we evaluate the network, we dont track the gradients\n",
    "    output = network(x_test) # We derive the output\n",
    "    test_loss = float(criterion(output, y_test)) # We calculate the loss\n",
    "    print(\"Test MSE:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbc0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
